{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN on MNIST\n",
    "\n",
    "\n",
    "We will  still use the same MNIST dataset with each example shaped  as `(28,28,1)` array.\n",
    "But this time no need to split into train/valid dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime as dt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import io\n",
    "import scipy\n",
    "\n",
    "data_home = '/tmp/scikit_learn_data/'\n",
    "datafile = '/tmp/mnist.npz'\n",
    "\n",
    "datapath = Path(datafile)\n",
    "if not(datapath.exists()):\n",
    "    print(\"Data File not found... downloading it\")\n",
    "    Xmnist, ymnist = fetch_openml('mnist_784',\n",
    "                                  version=1,\n",
    "                                  return_X_y=True,\n",
    "                                  data_home=data_home)\n",
    "    np.savez(datapath.as_posix(),\n",
    "             X=np.array(Xmnist, dtype='u8'),\n",
    "             y=np.array(ymnist, dtype='u8'))\n",
    "    print(\"Data File downloaded and saved\")\n",
    "    del Xmnist, ymnist\n",
    "\n",
    "print(\"Data File found... loading it into memory\")\n",
    "data = np.load(datapath.as_posix())\n",
    "Xmnist = data['X']/255.\n",
    "ymnist = keras.utils.to_categorical(data['y'])\n",
    "print(\"Data File loaded\")\n",
    "\n",
    "Xtrain, Ytrain = Xmnist[:60000], ymnist[:60000]\n",
    "Xtest, Ytest = Xmnist[-10000:], ymnist[-10000:]\n",
    "\n",
    "Xtrain = Xtrain.reshape((Xtrain.shape[0], 28, 28, 1))\n",
    "Xtest = Xtest.reshape((Xtest.shape[0], 28, 28, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN in KERAS\n",
    "\n",
    "\n",
    "GAN combined two networks a `generator` and a `discriminator`.\n",
    "Nevertheless,\n",
    "- while the generator is learnt the weights of the discrimnitor need to be fixed,\n",
    "- and while the discrimnitor is learnt the weights of the generator need also to be fixed.\n",
    "That's why there is no easy way to use the `fit` method proposed by keras.\n",
    "You had to write yourself the big training loop with the `train_on_batch` method.\n",
    "\n",
    "Here is an example code to show you how to combine the generator and the discriminator:\n",
    "```python\n",
    "    \n",
    "    generator = [...] # create your own generator\n",
    "    generator.build(input_shape=[...])\n",
    "    # no need to compile the generator as it will not be learnt by itself\n",
    "    generator.summary()\n",
    "\n",
    "    discriminator = [...] # create your own discriminator\n",
    "    discriminator.build(input_shape=[...])\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    discriminator.summary()\n",
    "\n",
    "    # we fixed discriminator weights\n",
    "    # must be call after discriminator compilation\n",
    "    # see definition in the next cell\n",
    "    set_trainable(discriminator, False) \n",
    "    \n",
    "    # Combined the generator and the discriminator into a gan model\n",
    "    # here the discriminator weights are fixed\n",
    "    gan = keras.Sequential(name='gan')\n",
    "    gan.add(generator)\n",
    "    gan.add(discriminator)\n",
    "    gan.build(input_shape=[...])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    gan.summary()\n",
    "\n",
    "```\n",
    "\n",
    "Now at each batch iteration:\n",
    "- the discriminator should be trained with fake examples labeled 0 and real examples labeled 1. \n",
    "- the gan should be trained with only fake examples labeled 1. \n",
    "\n",
    "You can produce fake example using the 'predict' method of the generator.\n",
    "\n",
    "```python\n",
    "\n",
    "    for epochs\n",
    "        for batch\n",
    "            #[...] \n",
    "            Xfake_batch = generator.predict(some_noise, verbose=0)\n",
    "            #[...]\n",
    "            # Xdiscriminator_batch contains both fake and real examples\n",
    "            # respectively labelled 0 and 1\n",
    "            discriminator.train_on_batch(Xdiscriminator_batch, Ydiscriminator_batch)\n",
    "            # Xgan_batch contains only fake examples\n",
    "            # labelled 1\n",
    "            gan.train_on_batch(Zgan_batch, Ygan_batch)\n",
    "            \n",
    "```\n",
    "\n",
    "\n",
    "In order to help you, the following function are given:\n",
    "- `set_trainable` to change the trainable state of a whole model\n",
    "- `batch_iterator` an iterator producing slice indices to split a dataset into batches\n",
    "\n",
    "See examples bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable(model, trainable=True):\n",
    "    model.trainable = trainable\n",
    "    for l in model.layers:\n",
    "       l.trainable = trainable\n",
    "\n",
    "def batch_iterator(nx, batch_size, shuffle=True):\n",
    "# nx : bumber of examples in the set\n",
    "# batch_size: the desired batch size\n",
    "# shuffle: whether to shuffle examples or not\n",
    "# It works even if nx is not divisible by batch_size\n",
    "    idx = np.arange(nx)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    n_batchs = int(np.floor(nx/batch_size))\n",
    "    if n_batchs * batch_size < nx:\n",
    "        n_batchs += 1\n",
    "    start = 0\n",
    "    for batch in range(n_batchs):\n",
    "        end = min(start + batch_size, nx)\n",
    "        aSlice = idx[start:end]\n",
    "        start = end\n",
    "        yield aSlice, batch, n_batchs\n",
    "# yield the current slice, the current batch index, and the total number of batches\n",
    "\n",
    "print(\"Batch iterator examples\")\n",
    "for aSlice, batch, n_batch in batch_iterator(100,10,shuffle=False):\n",
    "    print(aSlice, batch, n_batch)\n",
    "print(\"\")\n",
    "for aSlice, batch, n_batch in batch_iterator(100,10):\n",
    "    print(aSlice, batch, n_batch)\n",
    "print(\"\")\n",
    "for aSlice, batch, n_batch in batch_iterator(100,15,shuffle=False):\n",
    "    print(aSlice, batch, n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "1) Build a GAN to produce MNIST like images.\n",
    "\n",
    "2) Build a Pac-GAN to produce more diversity (2 images at the input of the discriminator).\n",
    "\n",
    "3) Build a Condition GAN to produce MNIST like images conditioned to the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
